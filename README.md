# Natural-Language-Processing

**Technology: Python, Google Collab.<br>
•	Programmed the bigram model with and without add-one Laplace smoothing for a given corpus.<br>
•	Computed positive pointwise mutual information for terms and context word with/out add-2 smoothing.<br>
•	Implemented HMM and the Viterbi algorithm to assign POS tags to given text.<br>
•	Trained and evaluated a feed-forward neural model on Reuters corpus using Google Collab and varying the parameters for best efficiency and accuracy.<br>
•	Implemented the CKY Parser for CNF Grammar on given text.<br>
•	Executed the constituency parser with a self-attentive encoder (Kitaev & Klein 2018) from Github.<br>
•	Implemented manual and automatic Semantic Role Labeling using ProbBank definitions and neural SRL code from Github on the given text.<br>
•	Implemented Simple LESK WSD algorithm to disambiguate the given text.<br>
•	Identified named entities, IOB notations, temporal expressions with normalization and relation extraction for given text.<br>**


HOMEWORK 1
  1. Regular Expressions
  2. N-Grams
  3. Vector Semantics
  4. Part-of-Speech Tagging
  
HOMEWORK 2
  1. CKY PARSER
  2. Statistical Parsing
  3. Semantic Role Labeling

HOMEWORK 3
  1. Simple LESK Word Sense Disambiguation
  2. Information Extraction
